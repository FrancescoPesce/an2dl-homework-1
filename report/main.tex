\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage{lipsum}
\usepackage{multicol}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{hyperref}
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\usepackage[left=2.00cm, right=2.00cm, top=2.00cm, bottom=2.00cm]{geometry}

\title{AN2DL Reports Template}

\begin{document}

\begin{figure}[H]
      \raggedright
      \includegraphics[scale=0.4]{polimi.png} \hfill
      \includegraphics[scale=0.3]{airlab.jpeg}
\end{figure}

\vspace{5mm}

\begin{center}
      % Select between First and Second
      {\Large \textbf{AN2DL - First Homework Report}}\\
      \vspace{2mm}
      % Change with your Team Name
      {\Large \textbf{LosPollosHermanos}}\\
      \vspace{2mm}
      % Team Members Information
      {\large Mohammadhossein Allahakbari,}
      {\large Michele Miotti,}
      {\large Francesco Pesce}\\
      \vspace{2mm}
      % Codabench Nicknames
      {mh2033,}
      {michelem,}
      {francescopesce}\\
      \vspace{2mm}
      % Matriculation Numbers
      {246639,}
      {249499,}
      {247974}\\
      \vspace{5mm}
      \today
\end{center}
\vspace{5mm}

\begin{multicols}{2}
      % Note: The following sections represent a suggested
      % structure. We don't need to follow it strictly.

      % -----------------------------------------------------------------------
      % INTRODUCTION
      % -----------------------------------------------------------------------
      \section{Introduction}
      % In this section, you should present your project's context and
      % objectives. You might want to:
      % \begin{itemize}
      %     \item Dehe problem (\textit{you may use italics to highlight
      %               definitions})
      %     \item State your goals (\textbf{emphasise key points with bold})
      %     \item Outline your approach
      % \end{itemize}

      % \noindent For instance, you might write: ``This project focuses on
      % \textit{image classification} using \textbf{deep learning} techniques."

      This project presents the results of the \textit{image classification}
      task proposed in the first homework of the Artificial Neural Networks and
      Deep Leaning course at Politecnico di Milano. The goal of the project is
      to classify blood cell images into 8 different classes by using deep
      learning techniques such as Convolutional Neural Networks (CNNs).

      % -----------------------------------------------------------------------
      % PROBLEM ANALYSIS
      % -----------------------------------------------------------------------
      \section{Problem Analysis}
      % Here you can discuss your initial analysis of the problem. Consider
      % including:
      % \begin{enumerate}
      %     % 8 classes, 96x96 rgb images, labels, etc
      %     \item Dataset characteristics
      %     \item Main challenges % The test set is horrible
      %     That the test set was not horrible? Is this what they mean?
      %     \item Initial assumptions 
      % \end{enumerate}

      % \noindent If you need to reference papers, use the citation command:
      % Recent work~\cite{lecun2015deep} suggests..."

      The dataset consists about 10,000 RGB images of blood cells, each
      of size 96\(\times\)96 pixels, to be classified into 8 different
      categories. After manually analyzing the dataset, we found that
      a part of it contained duplicate and misleading images that were doctored
      with unrelated overlays. We considered this a hint that the test set
      might contain similar images, enabling us to experiment with different
      techniques to improve the model's performance.

      Furthermore, the test set provided by the course instructors did not
      follow the same distribution as the training set. After using our local
      validation set to tune the model, we found that the model's performance
      on the Codabench platform was significantly lower.

      The classes within the dataset were imbalanced, with some classes having
      more samples than others. Through data augmentation techniques, we were
      able match a more balanced distribution of samples across the classes,
      but we found that this did not improve the model's performance.

      Consequently, we verified the possibility that the test set was
      distributed in a similar way to the training set, so the need to
      normalize the prior probabilities of the classes was not necessary.

      % -----------------------------------------------------------------------   
      % METHOD
      % -----------------------------------------------------------------------
      \section{Method}
      % Not sure what they are asking here. The final model?
      % This section should detail your approach. You can use equations to
      % explain your methodology. For example, a simple model representation:
      % \begin{equation}
      %     \label{eq:model}
      %     f(x) = \text{softmax}(Wx + b)
      % \end{equation}

      % \noindent Or a more complex loss function:
      % \begin{equation}
      %     \label{eq:loss}
      %     \mathcal{L} = -\frac{1}{N}\sum_{i=1}^{N} y_i\log(\hat{y}_i)
      % \end{equation}

      % \noindent Reference these equations in your text, like:``As shown in
      % equation~\ref{eq:model}..."

      After experimenting with different models and techniques (as detailed in
      the next section), we found that the best approach was to use a chain of
      convolutional and downsampling layers, followed by two custom blocks that
      combine the features of residual networks and inception blocks (we will
      explain this in detail in the next sections).

      The output of these custom blocks is then fed into a global average
      pooling layer, in order to reduce the dimensionality of the output. A
      hidden dense layer with 128 units and a following output layer with 8
      units and a softmax activation function are used to classify the images.

      % -----------------------------------------------------------------------
      % EXPERIMENTS
      % -----------------------------------------------------------------------
      \section{Experiments}
      % Step 1: bad CNN just to test things out
      % Step 2: normal augmentation
      % Step 3: overlaying
      % Step 4: keras cv (gigachad picture)
      % Step 5: testing residual networks (up to 0.8)
      %   and inception blocks (up to 0.72)
      % Step 6: custom block (in detail)
      % Step 7: add more filters
      % Step 8.....: not done yet
      % If you need accuracies, number of parameters etc ask me

      % Failed experiments:
      % transfer learning (bad accuracy) (VGG, MobileNet, ResNet), both with and
      % without fine tuning
      % Lion, stochastic gradient descent with momentum

      % Ideas that worked momentarily:
      % voting mechanism with multiple neural networks
      % averaging weights below a certain validation loss
      A number of experiments were conducted to get a higher accuracy on the
      test
      set. Some of the experiments were successful, while others were not. The
      following is a list of meaningful experiments that were conducted:

      \begin{itemize}
            \item \textbf{Simple CNN:} A simple Convolutional Neural Network
                  (CNN)
                  was implemented to test the dataset and the platform. This
                  step
                  performed well on our local training and validation sets, but
                  not
                  on the Codabench platform. After this step, we suspected that
                  the
                  test set was sampled from a different distribution.
            \item \textbf{Data Augmentation:} We augmented the dataset by
                  following the same techniques taught in class, but this did
                  not significantly improve the accuracy.
            \item \textbf{Overlaying:} After finding the doctored images in the
                  dataset, we hypothesized that the test set might contain
                  similar images. We then experimented with overlaying images
                  with similar overlays, improving the accuracy by about 0.3.
            \item \textbf{Keras CV:} By using Keras CV, we were able to achieve
                  an accuracy of 0.71. This was done by setting custom
                  augmentation pipelines. This way, transformations are applied
                  to the images during training, instead of pre-processing the
                  entire dataset in the beginning. Furthermore, transformations
                  are applied epoch by epoch, which helps the model generalize
                  better.
            \item \textbf{Residual Networks:} We experimented with Residual
                  Networks and Inception Blocks, achieving accuracies of 0.8
                  and
                  0.72, respectively.
            \item \textbf{Custom Block:} A custom block was implemented to
                  mix the features of the residual and inception blocks. This
                  is further explained in the next section.
                  % \item \textbf{More Filters:}
      \end{itemize}

      Some experiments that failed include transfer learning with VGG,
      MobileNet, and ResNet. Specifically, we used ImageNet weights with a
      custom-trained dense layer on top. We also tried fine-tuning the entire
      network, by training a variable number of layers. Unfortunately, these
      experiments did not prove to be successful.

      We also tried using the Lion optimizer and stochastic gradient descent
      with momentum. However, these	experiments did not yield satisfactory
      results. Particularly, after some experimentation, we found
      that the Lion optimizer had to be superseded by the Adam optimizer.

      Other failed experiments include a test-time augmentation with an array
      of different augmentations and selectively picking the most common
      prediction, an attempt to filter out noise using Fourier transforms, and
      averaging weights below a certain validation loss.

      Other ideas were successful at first, such as a voting mechanism with
      multiple neural networks. These ideas were not pursued further due to
      their unorthodox nature: as the project was a mean to learn from the
      course, we decided to approach the problem by following the methods
      taught in class.

      % For your experiments, you might want to present your results in tables.
      % Here's an example of a wide table comparing different models:

      % \begin{table*}[t]
      %     \centering
      %     \setlength{\tabcolsep}{3pt}
      %     \caption{An example of wide table. Best results are highlighted in
      %         \textbf{bold}.}
      %     \begin{tabularx}{\textwidth}{lYYYc}
      %         \toprule
      %         Model            & Accuracy                  & Precision
      %                          & Recall                    & ROC AUC
      %         \\
      %         \midrule
      %         VGG18            & 72.20 $\pm$ 3.06          & 94.95 $\pm$ 0.52
      %                          &
      %         86.95 $\pm$ 0.55 & 80.16 $\pm$ 0.81
      %         \\
      %         Custom Model     & 27.71 $\pm$ 3.19          & 75.70 $\pm$ 1.07
      %                          & 55.75 $\pm$ 2.16          & 36.60 $\pm$ 1.26
      %         \\
      %         ResNet18         & \textbf{89.24 $\pm$ 2.38} & \textbf{95.54
      %         $\pm$ 0.49}      & \textbf{93.43 $\pm$ 1.30} & \textbf{91.68 $\pm$
      %             0.71}
      %         \\
      %         \bottomrule
      %     \end{tabularx}
      %     \label{tab:Performance}
      % \end{table*}

      % \noindent For more specific measurements, you might use a narrower
      % table:

      % \begin{table}[H]
      %     \centering
      %     \setlength{\tabcolsep}{3pt}
      %     \caption{An example of table. Best results may be highlighted in
      %         \textbf{bold}.}
      %     \begin{tabularx}{\linewidth}{lY}
      %         \toprule
      %         Time [$\mu$s] & Distance [mm] \\
      %         \midrule
      %         22$\pm$4      & 8$\pm$1       \\
      %         17$\pm$3      & 7$\pm$1       \\
      %         15$\pm$3      & 6$\pm$1       \\
      %         13$\pm$2      & 5$\pm$1       \\
      %         10$\pm$2      & 4$\pm$1       \\
      %         8$\pm$2       & 3$\pm$1       \\
      %         5$\pm$1       & 2$\pm$1       \\
      %         37$\pm$1      & 1$\pm$1       \\
      %         \bottomrule
      %     \end{tabularx}
      %     \label{tb:Measurements}
      % \end{table}

      % \noindent You can also include figures to visualise your results:
      % \begin{figure}[H]
      %     \centering
      %     \includegraphics[width=0.75\linewidth]{random.jpeg}
      %     \caption{Example figure showing [describe what the figure shows]}
      %     \label{fig:results}
      % \end{figure}

      % \noindent Reference figures using like:``As shown in
      % Figure~\ref{fig:results}..."

      % -----------------------------------------------------------------------
      % CUSTOM BLOCK
      % -----------------------------------------------------------------------
      \section{Custom Block}
      % Explain the custom block in detail

      Two of the most successful experiments were the residual networks and
      inception blocks. The residual networks were used to inject skip
      connections into the network, which helped it learn the residuals of the
      input. An accuracy of 0.8 was achieved using this technique.

      Introducing inception blocks was another idea that ultimately helped us
      achieve an accuracy of 0.72. A set of differently sized filters were
      applied to the input, and the results were concatenated to form the
      output of the block. This helped the network capture both fine-grained
      and coarse-grained features of the dataset.

      As opposed to previous experiments that seemed to guess correctly or
      incorrectly in a similarly distributed fashion with respect to each
      other, the residual networks and inception blocks were able to guess in a
      more independent way. This led us to hypothesize that their learning
      target captured distinct features of the dataset, and could be combined
      to improve the accuracy even further.

      The custom block was therefore implemented by combining the residual and
      inception blocks by constructing the residual block's model and
      substituting the convolutional layers with the inception block's
      convolutional layers.

      % -----------------------------------------------------------------------
      % RESULTS
      % -----------------------------------------------------------------------
      \section{Results}
      % Keras CV is good
      % Combining residual and inception is good
      % Present your main findings here. You might want to:
      % \begin{itemize}
      %     \item Compare your results with baselines
      %     \item Highlight key achievements using \textbf{bold text}
      %     \item Explain any unexpected outcomes
      % \end{itemize}

      % -----------------------------------------------------------------------
      % DISCUSSION
      % -----------------------------------------------------------------------
      \section{Discussion}
      % Wait until we have the final results
      % In this section, analyse your results critically. Consider:
      % \begin{itemize}
      %     \item Strengths and weaknesses
      %     \item Limitations and assumptions
      % \end{itemize}

      % -----------------------------------------------------------------------
      % CONTRIBUTIONS
      % -----------------------------------------------------------------------
      \section{Contributions}

      Most ideas and propositions were evenly distributed among the team
      members, while actual coding was more specific to each member's
      strengths. A number of branches were created in the repository to
      experiment with different ideas and techniques. The final model was a
      result of combining the best experiments from each branch.

      % -----------------------------------------------------------------------
      % CONCLUSIONS
      % -----------------------------------------------------------------------
      \section{Conclusions}
      % Summarise your work and discuss potential future directions. This is
      % where you can:
      % \begin{itemize}
      %     \item Restate main contributions
      %     \item Suggest improvements
      %     \item Propose future work
      % \end{itemize}

      Overall, the project's challenge-like nature has helped us follow a
      structured approach to solving the problem. Not expecting a pristine test
      set, we were able to experiment with most of the techniques we learned in
      class, instad of immediately finding the best solution.

      % Remember to include the bibliography!
      \bibliography{references}
      \bibliographystyle{abbrv}

\end{multicols}
\end{document}