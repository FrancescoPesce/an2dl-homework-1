\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage{lipsum}
\usepackage{multicol}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{hyperref}
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\usepackage[left=2.00cm, right=2.00cm, top=2.00cm, bottom=2.00cm]{geometry}

\title{AN2DL Reports Template}

\begin{document}

\begin{figure}[H]
      \raggedright
      \includegraphics[scale=0.4]{polimi.png} \hfill
      \includegraphics[scale=0.3]{airlab.jpeg}
\end{figure}

\vspace{5mm}

\begin{center}
      % Select between First and Second
      {\Large \textbf{AN2DL - First Homework Report}}\\
      \vspace{2mm}
      % Change with your Team Name
      {\Large \textbf{LosPollosHermanos}}\\
      \vspace{2mm}
      % Team Members Information
      {\large Mohammadhossein Allahakbari,}
      {\large Michele Miotti,}
      {\large Francesco Pesce}\\
      \vspace{2mm}
      % Codabench Nicknames
      {mh2033,}
      {michelem,}
      {francescopesce}\\
      \vspace{2mm}
      % Matriculation Numbers
      {246639,}
      {249499,}
      {247974}\\
      \vspace{5mm}
      \today
\end{center}
\vspace{5mm}

\begin{multicols}{2}
      % Note: The following sections represent a suggested
      % structure. We don't need to follow it strictly.

      % -----------------------------------------------------------------------
      % INTRODUCTION
      % -----------------------------------------------------------------------
      \section{Introduction}
      % In this section, you should present your project's context and
      % objectives. You might want to:
      % \begin{itemize}
      %     \item Dehe problem (\textit{you may use italics to highlight
      %               definitions})
      %     \item State your goals (\textbf{emphasise key points with bold})
      %     \item Outline your approach
      % \end{itemize}

      % \noindent For instance, you might write: ``This project focuses on
      % \textit{image classification} using \textbf{deep learning} techniques."

      This report presents the results of the \textit{image classification}
      task proposed in the first homework of the Artificial Neural Networks and Deep Learning course at Politecnico di Milano. The goal of the project is to classify blood cell images into 8 different classes by using deep learning techniques such as Convolutional Neural Networks (CNNs).\cite{NIPS1989_53c3bce6}

      % -----------------------------------------------------------------------
      % PROBLEM ANALYSIS
      % -----------------------------------------------------------------------
      \section{Problem Analysis}
      % Here you can discuss your initial analysis of the problem. Consider
      % including:
      % \begin{enumerate}
      %     % 8 classes, 96x96 rgb images, labels, etc
      %     \item Dataset characteristics
      %     \item Main challenges % The test set is horrible
      %     That the test set was not horrible? Is this what they mean?
      %     \item Initial assumptions 
      % \end{enumerate}

      % \noindent If you need to reference papers, use the citation command:
      % Recent work~\cite{lecun2015deep} suggests..."

      The task involved the use of three different datasets, only one of which was available to us. The others were used by the Codabench platform to evaluate the developed models, with different datasets used in the Development and Final phases.

      The provided dataset consisted in around 10,000 RGB images of blood cells, each of size 96\(\times\)96 pixels, to be classified into 8 different categories. We modeled the problem as a multi-label classification problem, where each image is an observation of a random variable with an unknown distribution, dependent on its label, and labels themselves are distributed in an unknown way.
      
      After manually analyzing the dataset, we found that a part of it contained very clear outliers, i.e. duplicate and misleading images that were doctored by adding unrelated overlays. As such, we decided to remove said images from the dataset.

      Furthermore, after testing our first model on a local test set, using images taken from the dataset we received, we found that the model's performance on the Codabench platform~\cite{codabench} was significantly lower. The fact that this mismatch appeared with multiple models meant that images in the dataset present on Codabench could not be reasonably assumed to have been generated by the same distribution that generated the images we were provided, and was likely doctored in some way.

      The classes within the dataset were imbalanced, up to a 2.75x imbalance after removing doctored images. Multiple experiments, such as using data  augmentation techniques to balance out the number of samples for each class in the training set, seemed to indicate that the distribution of labels in the closed datasets was the same as in the provided one. As such, there was no need to adjust the prior distribution of the labels.

      % -----------------------------------------------------------------------   
      % METHOD
      % -----------------------------------------------------------------------
      \section{Method}
      % Not sure what they are asking here. The final model?
      % This section should detail your approach. You can use equations to
      % explain your methodology. For example, a simple model representation:
      % \begin{equation}
      %     \label{eq:model}
      %     f(x) = \text{softmax}(Wx + b)
      % \end{equation}

      % \noindent Or a more complex loss function:
      % \begin{equation}
      %     \label{eq:loss}
      %     \mathcal{L} = -\frac{1}{N}\sum_{i=1}^{N} y_i\log(\hat{y}_i)
      % \end{equation}

      % \noindent Reference these equations in your text, like:``As shown in
      % equation~\ref{eq:model}..."

      Most of the development process was done on a Jupyter notebook, which was used to load and analyse the data, apply augmentation techniques, define and train the models, and generate and test the submissions for the Codabench platform, using the Tensorflow and Keras libraries for Python. Most of the models were trained locally, due to strict usage limits of popular platforms, such as Google Colab.

      We initially split our dataset into a training, validation and test sets, but upon noticing that the accuracy on the local test set was unrelated to the accuracy on Codabench, we decided to get rid of it, and split the dataset into just training and validation sets, with a 80-20 split.

      Since the problem consists in classification of images, all the produced models contain a convolutional CNN core, due to two important properties of CNNs: automatic feature extraction and sparsity of the weights with respect to dense neural networks. After feature extraction, the features are utilized by a dense neural network, with 8 output neurons, and a softmax activation. Since this is a multi-class classification problem, the loss function we employed was categorical crossentropy, as minimizing said loss function maximizes the likelihood of producing the data, under some distribution assumptions.

      After experimenting with different models and techniques, as detailed in Section \ref{sec:experiments}, we found that the best approach was to use a chain of two convolutional and downsampling layers, followed by two instances of a custom block that combines the features of residual networks and inception blocks, as illustrated in Section \ref{sec:custom_block}.

      The output of these custom blocks is then fed into a global average
      pooling layer, in order to reduce the dimensionality of the output and as a regularization technique. The dense network at the top of the net has a hidden layer, followed by a Dropout layer for regularization. Moreover, each convolutional layer is followed by a Batch Normalization layer. THe model uses the Adam optimizer with early stopping after 40 epochs of stagnation of validation accuracy, 1e-3 initial learning rate and adaptively lowering the learning rate 5-fold after 20 epochs of stagnation in validation loss.

      The final network has 1.9 million parameters, achieved an accuracy of 88.4\% on the final dataset on Codabench, and was developed after exploring a complex design space formed by the number of custom and convolutional blocks, presence of the hidden dense layer, optimizer used (e.g. Lion or Adam), learning rate and others.

      % -----------------------------------------------------------------------
      % EXPERIMENTS
      % -----------------------------------------------------------------------
      \label{sec:experiments}
      \section{Experiments}
      % Step 1: bad CNN just to test things out
      % Step 2: normal augmentation
      % Step 3: overlaying
      % Step 4: keras cv (gigachad picture)
      % Step 5: testing residual networks (up to 0.8)
      %   and inception blocks (up to 0.72)
      % Step 6: custom block (in detail)
      % Step 7: add more filters
      % Step 8.....: not done yet
      % If you need accuracies, number of parameters etc ask me

      % Failed experiments:
      % transfer learning (bad accuracy) (VGG, MobileNet, ResNet), both with and
      % without fine tuning
      % Lion, stochastic gradient descent with momentum

      % Ideas that worked momentarily:
      % voting mechanism with multiple neural networks
      % averaging weights below a certain validation loss
      \subsection{Successful experiments}

      A number of experiments aimed at increasing the accuracy on Codabench were conducted. The following is a list of experiments, in chronological order, that significantly improved the accuracy of the model. For each experiment, multiple models were tested, and the best accuracy obtained using each technique is discussed in Section 
      \ref{sec:results}.

      \begin{itemize}
            \item \textbf{Simple CNN:} A simple CNN based on the laboratory lectures of the course was developed to test the development flow and the platform. As previously stated, this simple experiment made us realize that the Codabench dataset was likely generated by a different distribution with respect to the provided one.
            \item \textbf{Data Augmentation:} We implemented simple image augmentation techniques seen during the laboratory lectures.
            \item \textbf{Overlaying:} After finding the doctored images in the dataset, we hypothesized that the test set may contain images doctored in the same way. Therefore, we experimented with applying overlays to the images in our dataset. We tested different image overlaying techniques, such as multiplying the 
            normalized pixel values and using convex combinations of the images.
            \item \textbf{Keras CV:} Using the KerasCV library\cite{chollet2015keras} provided the first satisfactory results. Compared to previous data augmentation techniques, the library provided richer augmentations, which could be applied in a complex pipeline. An important feature of KerasCV is that augmentations are applied during training, and not to the whole dataset before training. This allows for augmentations to use different seeds, fully using the whole information of the training set. Augmentation was applied to both training and validation sets, as the augmented images were likely more "similar" to the images on Codabench, and overlays were removed due to poor compatibility with the new augmentations.
            \item \textbf{Richer architectures:} We tried experimenting with more complex CNNs, using techniques employed by popular past CNNs, including Inception blocks\cite{Inception}, Residual blocks\cite{He2015DeepRL}, Global Average Pooling\cite{Lin2013NetworkIN} and Batch Normalization layers\cite{pmlr-v37-ioffe15}.
            \item \textbf{Custom Block:} A custom block was implemented to mix the features of the residual and inception blocks. The rationale and more details are discussed in section \ref{sec:custom_block}.
      \end{itemize}

      \subsection{Other experiments}

      A failed experiment that merits explicit mention is using transfer learning\cite{TransferLearning}. We tried transfer learning from some well-known CNN architectures, available as Keras applications, such as VGG19, ResNet50V2 and MobileNetV2. For all architectures, we used the "ImageNet"\cite{ImageNet} pre-trained weights, removed the dense network and added a custom one. We attempted both training just the dense network, freezing the weights of the feature extraction network, and unfreezing the top convolutional layers, but achieved very poor results with both techniques. Due to the poor initial results, the fact that many pretrained models we large, causing issues with the amount of memory locally available and the memory issues of the Codabench platform, and the fact that we wished to create our custom CNN from scratch, we abondoned this avenue.

      Another failed experiment is using the Lion and stochastic gradient descent with momentum optimiers provided by Keras. We attempted to use these optimizers multiple times, but the results were always sub-par compared to the Adam\cite{Kingma2014AdamAM} optimizer. Other failed experiments include performing test-time agumentation, an attempt to filter out noise in the Codabench images using Fourier transforms and filters, in case they contained noise, and mixed precision.

      Other ideas were successful at first, such as averaging weights from all epochs below a certain validation loss as a regularization technique, and a voting mechanism with multiple neural networks, acting as a sort of ensemble method. The first idea was effective on early models, but did no longer have an effect on more fine-tuned ones, and the second one was scrapped for two reasons: the larger memory and computation requirements and its unorthodox nature.

      % For your experiments, you might want to present your results in tables.
      % Here's an example of a wide table comparing different models:

      % \begin{table*}[t]
      %     \centering
      %     \setlength{\tabcolsep}{3pt}
      %     \caption{An example of wide table. Best results are highlighted in
      %         \textbf{bold}.}
      %     \begin{tabularx}{\textwidth}{lYYYc}
      %         \toprule
      %         Model            & Accuracy                  & Precision
      %                          & Recall                    & ROC AUC
      %         \\
      %         \midrule
      %         VGG18            & 72.20 $\pm$ 3.06          & 94.95 $\pm$ 0.52
      %                          &
      %         86.95 $\pm$ 0.55 & 80.16 $\pm$ 0.81
      %         \\
      %         Custom Model     & 27.71 $\pm$ 3.19          & 75.70 $\pm$ 1.07
      %                          & 55.75 $\pm$ 2.16          & 36.60 $\pm$ 1.26
      %         \\
      %         ResNet18         & \textbf{89.24 $\pm$ 2.38} & \textbf{95.54
      %         $\pm$ 0.49}      & \textbf{93.43 $\pm$ 1.30} & \textbf{91.68 $\pm$
      %             0.71}
      %         \\
      %         \bottomrule
      %     \end{tabularx}
      %     \label{tab:Performance}
      % \end{table*}

      % \noindent For more specific measurements, you might use a narrower
      % table:

      % \begin{table}[H]
      %     \centering
      %     \setlength{\tabcolsep}{3pt}
      %     \caption{An example of table. Best results may be highlighted in
      %         \textbf{bold}.}
      %     \begin{tabularx}{\linewidth}{lY}
      %         \toprule
      %         Time [$\mu$s] & Distance [mm] \\
      %         \midrule
      %         22$\pm$4      & 8$\pm$1       \\
      %         17$\pm$3      & 7$\pm$1       \\
      %         15$\pm$3      & 6$\pm$1       \\
      %         13$\pm$2      & 5$\pm$1       \\
      %         10$\pm$2      & 4$\pm$1       \\
      %         8$\pm$2       & 3$\pm$1       \\
      %         5$\pm$1       & 2$\pm$1       \\
      %         37$\pm$1      & 1$\pm$1       \\
      %         \bottomrule
      %     \end{tabularx}
      %     \label{tb:Measurements}
      % \end{table}

      % \noindent You can also include figures to visualise your results:
      % \begin{figure}[H]
      %     \centering
      %     \includegraphics[width=0.75\linewidth]{random.jpeg}
      %     \caption{Example figure showing [describe what the figure shows]}
      %     \label{fig:results}
      % \end{figure}

      % \noindent Reference figures using like:``As shown in
      % Figure~\ref{fig:results}..."

      % -----------------------------------------------------------------------
      % CUSTOM BLOCK
      % -----------------------------------------------------------------------
      \label{sec:custom_block}
      \section{Custom Block}
      % Explain the custom block in detail

      Residual networks revolutionized CNNs by allowing to train deeper networks. The residual blocks inject skip connections into the network, which help the learning process by making deeper levers learn residuals with respect to the identity function. Adding said blocks to a simple CNN design significantly improved the accuracy of our models, up to 0.8 on Codabench.

      In inception blocks, a set of differently sized filters are applied to the block's input, and the results are concatenated as output channels. This helps the network capture both fine-grained and coarse-grained features of the dataset. However, adding inception blocks to the same design did not significantly vary our accuracy, obtaining a 0.72 on Codabench.

      As opposed to previous experiments, were each model classified correctly most of the same images as its predecessors plus some more, networks containing just residual blocks and just inception blocks were able to correctly classify different images. This led us to hypothesize that the models captured distinct features of the dataset, and could be combined to improve the accuracy even further.

      By combining the two blocks, we created what, to the best of our knowledge, is a new custom block. It is formed by constructing a residual block's model and substituting each convolutional layer and its activation with an inception block, as shown in Figure \ref{fig:custom_block}.

      \begin{figure}[H]
            \centering
            \includegraphics[width=0.75\linewidth]{custom_block.jpg}
            \caption{The structure of our custom block. TODO: set the correct image.}
            \label{fig:custom_block}
      \end{figure}

      % -----------------------------------------------------------------------
      % RESULTS
      % -----------------------------------------------------------------------
      \label{sec:results}
      \section{Results}
      % Keras CV is good
      % Combining residual and inception is good
      % Present your main findings here. You might want to:
      % \begin{itemize}
      %     \item Compare your results with baselines
      %     \item Highlight key achievements using \textbf{bold text}
      %     \item Explain any unexpected outcomes
      % \end{itemize}

      During the Development phase, we trained and submitted more than 50 models. Some of the models were very similar, differing for example in some hyperparameter, while others caused unsatisfactory results. Table \ref{tab:results} shows the best result using each of the techniques discussed in Section \ref{sec:experiments}, before introducing the next technique. Results marked with * are approximate, due to being obtained before the submissions reset to the Codabench platform after a couple of days of development.

      \begin{table}[H]
          \centering
          \setlength{\tabcolsep}{3pt}
          \begin{tabularx}{\linewidth}{lY}
              \toprule
              Experiment & Accuracy \\
              \midrule
              Simple CNN & 0.20* \\
              Data Augmentation & 0.25* \\
              Overlaying & 0.53 \\
              KerasCV & 0.71 \\
              Richer architectures & 0.80 \\
              Custom Block & 0.88 \\
              \bottomrule
          \end{tabularx}
          \label{tab:results}
      \end{table}

      % -----------------------------------------------------------------------
      % DISCUSSION
      % -----------------------------------------------------------------------
      \section{Discussion}
      % Wait until we have the final results
      % In this section, analyse your results critically. Consider:
      % \begin{itemize}
      %     \item Strengths and weaknesses
      %     \item Limitations and assumptions
      % \end{itemize}

      Due to the heavy usage of augmentation and regularization techniques, the model generalizes well the information learned from the training set. In fact, for all models trained using KerasCV, training accuracy is lower than validation accuracy, which is lower than the accuracy on the Codabench platform, due to the high variability introduced by augmentations. The usage of a model trained on commodity hardware means that the model's weight and performance and memory requirements during prediction are lower than most models obtained by transfer learning. Moreover, since the model is by no means lightweight, we propose a scaling technique for devices with lower capabilites. By simply substituting one of the custom blocks with a convolutional layer, and reducing the number of filters in each block, a lighter model by more than an order of magnitude in terms of number of parameters can be obtained, still achieving a respectable 0.82 accuracy on Codabench. Some of the model's limitations are the fact that the design space for hyperparameters, optimizers, number of filters and choice of augmentation techniques is so large that the optimal model was likely not found, and the fact that some larger models, such as ones obtained by transfer learning, if trained properly, may have been able to capture more features of the images and achieve higher accuracy.

      % -----------------------------------------------------------------------
      % CONTRIBUTIONS
      % -----------------------------------------------------------------------
      \section{Contributions}

      Most ideas and propositions were evenly distributed among the team
      members, while actual coding was more specific to each member's
      strengths. Many different branches were created in the repository to
      experiment with different ideas and techniques. The final model was a
      result of combining the best experiments from each branch.

      % -----------------------------------------------------------------------
      % CONCLUSIONS
      % -----------------------------------------------------------------------
      \section{Conclusions}
      % Summarise your work and discuss potential future directions. This is
      % where you can:
      % \begin{itemize}
      %     \item Restate main contributions
      %     \item Suggest improvements
      %     \item Propose future work
      % \end{itemize}

      In this project, we developed and trained a custom CNN, using techniques from many CNNs in the recent scientific literature. The model achieved an accuracy of 0.88 on the final dataset on the Codabench platform, and makes use of a custom block we developed by reasoning about the properties of the blocks we were shown in class. 

      Future work could include a more thorough analysis of the design space for the model, especially when it comes to the specific augmentation techniques, which seem to have a large effect on accuracy, and further attempts at transfer learning, which, if successful, could result in a comparison between our custom model and said models.

      Overall, the project's challenge-like nature has helped us follow a
      structured approach to solving the problem. The lack of a pristine test set gave us the opportunity to test out many different techniques and apply what was taught in class, instead of quickly finding a satisfactory result. 

      % Remember to include the bibliography!
      \bibliography{references}
      \bibliographystyle{abbrv}

\end{multicols}
\end{document}